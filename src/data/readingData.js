// Auto-generated from Google Sheets on 2025-08-19T22:21:00.765744
// Do not edit this file manually - changes will be overwritten

export const allSampleData = [
  {
    "id": "how_to_lie_with_statistics",
    "title": "How To Lie With Statistics",
    "author": "Darrell Huff",
    "link": "https://www.amazon.de/How-Lie-Statistics-Darrell-Huff/dp/0393310728",
    "type": "Book",
    "platform": "",
    "language": "English",
    "tags": [
      "data science",
      "statistics",
      "mathematics",
      "analytical thinking"
    ],
    "status": "read",
    "rating": 5,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Medium",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "4/3/2025",
    "dateRead": "03/01/2025"
  },
  {
    "id": "the_art_of_statistics:_how_to_learn_from_data",
    "title": "The Art of Statistics: How to Learn from Data",
    "author": "David Spiegelhalter",
    "link": "https://www.penguin.co.uk/books/294857/the-art-of-statistics-by-spiegelhalter-david/9780241258767",
    "type": "Book",
    "platform": "",
    "language": "English",
    "tags": [
      "data science",
      "statistics",
      "mathematics",
      "analytical thinking",
      "reading from data"
    ],
    "status": "in-progress",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Medium",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "7/24/2025",
    "dateRead": ""
  },
  {
    "id": "mahabir_pun",
    "title": "Mahabir Pun",
    "author": "Mahahir Pun",
    "link": "https://bookslandnepal.com/product/mahabir-pun",
    "type": "Book",
    "platform": "",
    "language": "English",
    "tags": [
      "biography",
      "life",
      "struggle",
      "inspiration"
    ],
    "status": "read",
    "rating": 4,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Long",
    "difficulty": "",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "11/13/2024",
    "dateRead": "11/01/2024"
  },
  {
    "id": "calling_bullshit:_the_art_of_skepticism_in_a_data_",
    "title": "Calling Bullshit: The Art of Skepticism in a Data-Driven World",
    "author": "Carl T. Bergstrom, Jevin D. West",
    "link": "https://www.amazon.com/Calling-Bullshit-Skepticism-Data-Driven-World/dp/0525509186",
    "type": "Book",
    "platform": "",
    "language": "English",
    "tags": [
      "statistics",
      "analytics",
      "data science",
      "mathematics",
      "analytical thinking"
    ],
    "status": "to-read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "",
    "difficulty": "Advanced",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "8/14/2025",
    "dateRead": ""
  },
  {
    "id": "breaking_the_sorting_barrier_for_directed_single_s",
    "title": "Breaking the Sorting Barrier for Directed Single-Source Shortest\r\nPaths",
    "author": "Ran Duan ∗,\r\nJiayi Mao ∗, Xiao Mao †,  Xinkai Shu, ‡ Longhui Yin ∗",
    "link": "https://arxiv.org/pdf/2504.17033",
    "type": "Academic Papers",
    "platform": "",
    "language": "English",
    "tags": [
      "algorithm",
      "analysis",
      "Dijkstra's algorithm",
      "shortest path",
      "record break"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "The first result to break the O(m + n log n) time bound of Dijkstra’s algorithm on sparse graphs showing that it is not optimal for SSSP. Gives a deterministic O(m log2/3 n)-time algorithm for single-source shortest paths (SSSP) on directed graphs with real non-negative edge weights in the comparison-addition model\n\n1. Finds key vertices to reduce the frontiers (set of vertices you have reached in the current layer)\n\n2. Uses Bellman-Ford-style relaxations (\"checking if the currently known distance to a vertex can be improved by going through another vertex and if so, update it\" - ChatGPT)\n\n3. Performs partial Bread First Search\n\nThis algorithm doesn't track all the vertices, it only keeps a subset of these vertices.",
    "quotes": [],
    "impact": null,
    "length": "",
    "difficulty": "Advanced",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "8/15/2025",
    "dateRead": "08/15/2025"
  },
  {
    "id": "statistical_methods:_basic_concepts,_interpretatio",
    "title": "Statistical methods: Basic concepts, interpretations, and cautions",
    "author": "Sander Greenland, Professor Emeritus \r\nDepartment of Epidemiology and Department of Statistics \r\nUniversity of California, Los Angeles",
    "link": "https://arxiv.org/pdf/2508.10168",
    "type": "Academic Papers",
    "platform": "",
    "language": "English",
    "tags": [
      "statistics",
      "inference",
      "interpretation",
      "hypothesis testing",
      "confidence"
    ],
    "status": "in-progress",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "08/18/2025",
    "dateRead": ""
  },
  {
    "id": "ai_and_memory_wall",
    "title": "AI and Memory Wall",
    "author": "Amir Gholami1,2 Zhewei Yao1 Sehoon Kim1 Coleman Hooper1 Michael W. Mahoney1,2,3 Kurt Keutzer1\r\n1University of California, Berkeley 2\r\nICSI 3LBNL",
    "link": "https://arxiv.org/pdf/2403.14123",
    "type": "Academic Papers",
    "platform": "",
    "language": "English",
    "tags": [
      "ai",
      "memory bound",
      "bottleneck",
      "run-time",
      "decoder models",
      "deep learning"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "1. One needs to study the arithmetic intensity of the operations involved. Arithmetic intensity is the number of FLOPs that can be performed per byte loaded from memory. Computed by dividing the total number of FLOPs by the total number of bytes accessed (also referred to as MOPs, or memory operations).\n2. Memory wall can become a major bottleneck for decoder models (at low batch sizes) and not compute.",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "",
    "relatedTo": [],
    "personalReflection": "Memory wall/bounds with large batch-size lead to sharp minimum, that is, spiky loss curves and may reduce generalization.",
    "citationFormat": "",
    "dateAdded": "08/19/2025",
    "dateRead": "08/19/2025"
  },
  {
    "id": "breaking_the_memory_wall:_a_study_of_i/o_patterns_",
    "title": "Breaking the Memory Wall: A Study of I/O Patterns and GPU\r\nMemory Utilization for Hybrid CPU-GPU Offloaded Optimizers",
    "author": "Avinash Maurya, \nJie Ye, M. Mustafa Rafique,\nFranck Cappello,\nBogdan Nicolae,",
    "link": "https://arxiv.org/pdf/2406.10728v1",
    "type": "Academic Papers",
    "platform": "",
    "language": "English",
    "tags": [
      "optimization",
      "memory-bound",
      "deep learning",
      "ai",
      "llms"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "It has revealed several important bottlenecks of ex\u0002isting implementations: \n\n1. redundant and/or blocking device-to-host and host-to-device data transfers that slow down the last backward pass and the update step, \n\n2. slow upload of updated model parameters back to the GPUs in half precision, \n\n3. fluctuations of GPU memory utilization and PCIe links that lead to underutilization of GPUs especially during the update step, \n\n4. contention for PCIe links due to 3D parallelism, and contention for the I/O bandwidth of the host memory due to concurrent data transfers and CPU computations.",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "08/19/2025",
    "dateRead": "08/19/2025"
  },
  {
    "id": "scheduling_deep_learning_jobs_in_multi_tenant_gpu_",
    "title": "Scheduling Deep Learning Jobs in Multi-Tenant GPU Clusters via Wise Resource Sharing",
    "author": "Yizhou Luo∗\r\n, Qiang Wang†\r\n, Shaohuai Shi†\r\n, Jiaxin Lai∗\r\n, Shuhan Qi†\r\n, Jiajia Zhang†\r\n, Xuan Wang†\r\nHarbin Institute of Technology (Shenzhen)\r\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies",
    "link": "https://arxiv.org/pdf/2407.13088",
    "type": "",
    "platform": "",
    "language": "English",
    "tags": [
      "deep learnig",
      "job scheduling",
      "memory contention",
      "resource utilization"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "1. Introduces a novel DDL job scheduling model enabling multiple jobs to fully or partially share the same set of GPUs while ensuring model convergence through gradi\u0002ent accumulation. Unlike existing methods that increase batch size and GPU numbers to enhance performance, risking accuracy degradation, our model focuses on GPU resource sharing across jobs and mitigates GPU memory constraints through gradient accumulation, thereby poten\u0002tially reducing queuing time for waiting DDL jobs.\n\n2. SJF-BSBF (shortest job first with best shar\u0002ing benefit first) reduces average job com\u0002pletion time by 27-33%.  \n\n3. Given that GPU memory constraints may limit the per-GPU batch size, some schedulers tackle this limitation through memory offloading (which may introduce additional system overhead) or by adjusting batch sizes and other training hyper-parameters (which may compromise model accuracy).\n\n4. Adopts the ”gang-scheduling” discipline widely prevalent in practical large-scale GPU clusters.\n\n5. Different jobs exhibit varying sensitivities to network communication and GPU workloads.\n\n6. Once the free GPU number is not enough to execute the job, they manage to look for those already occupied by the running jobs to schedule the new one. This is the core logic of SJF-BSBF.",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "GPU contention can negatively impact model convergence and final accuracy. The authors have shown that this can be mitigated using a non-preemptive scheduling heuristic (SJF-BSBF) that dynamically adjusts sub-batch sizes and sharing timings while ensuring both efficient GPU sharing and maintenance of Deep Learning model accuracy via gradient accumulation.",
    "citationFormat": "",
    "dateAdded": "08/19/2025",
    "dateRead": "08/19/2025"
  }
];

export const getReadingStats = (data) => {
  const readItems = data.filter(item => item.status === 'read').length;
  const inProgress = data.filter(item => item.status === 'in-progress').length;
  const toRead = data.filter(item => item.status === 'to-read').length;
  
  const ratedItems = data.filter(item => item.rating !== null);
  const averageRating = ratedItems.length > 0 
    ? (ratedItems.reduce((sum, item) => sum + item.rating, 0) / ratedItems.length).toFixed(1)
    : 0;
    
  const impactItems = data.filter(item => item.impact !== null);
  const averageImpact = impactItems.length > 0
    ? (impactItems.reduce((sum, item) => sum + item.impact, 0) / impactItems.length).toFixed(1)
    : 0;

  return {
    totalItems: data.length,
    readItems,
    inProgress,
    toRead,
    averageRating: parseFloat(averageRating),
    averageImpact: parseFloat(averageImpact)
  };
};

export const getTagFrequency = (data) => {
  const tagCounts = {};
  data.forEach(item => {
    item.tags.forEach(tag => {
      tagCounts[tag] = (tagCounts[tag] || 0) + 1;
    });
  });
  return tagCounts;
};
