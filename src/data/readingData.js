// Auto-generated from Google Sheets on 2025-08-21T22:21:48.201331
// Do not edit this file manually - changes will be overwritten

export const allSampleData = [
  {
    "id": "how_to_lie_with_statistics",
    "title": "How To Lie With Statistics",
    "author": "Darrell Huff",
    "link": "https://www.amazon.de/How-Lie-Statistics-Darrell-Huff/dp/0393310728",
    "type": "book",
    "platform": "",
    "language": "English",
    "tags": [
      "data science",
      "statistics",
      "mathematics",
      "analytical thinking"
    ],
    "status": "read",
    "rating": 5,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Medium",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "4/3/2025",
    "dateRead": "03/01/2025"
  },
  {
    "id": "the_art_of_statistics:_how_to_learn_from_data",
    "title": "The Art of Statistics: How to Learn from Data",
    "author": "David Spiegelhalter",
    "link": "https://www.penguin.co.uk/books/294857/the-art-of-statistics-by-spiegelhalter-david/9780241258767",
    "type": "book",
    "platform": "",
    "language": "English",
    "tags": [
      "data science",
      "statistics",
      "mathematics",
      "analytical thinking",
      "reading from data"
    ],
    "status": "in-progress",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Medium",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "7/24/2025",
    "dateRead": ""
  },
  {
    "id": "mahabir_pun",
    "title": "Mahabir Pun",
    "author": "Mahahir Pun",
    "link": "https://bookslandnepal.com/product/mahabir-pun",
    "type": "book",
    "platform": "",
    "language": "English",
    "tags": [
      "biography",
      "life",
      "struggle",
      "inspiration"
    ],
    "status": "read",
    "rating": 4,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Long",
    "difficulty": "",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "11/13/2024",
    "dateRead": "11/01/2024"
  },
  {
    "id": "calling_bullshit:_the_art_of_skepticism_in_a_data_",
    "title": "Calling Bullshit: The Art of Skepticism in a Data-Driven World",
    "author": "Carl T. Bergstrom, Jevin D. West",
    "link": "https://www.amazon.com/Calling-Bullshit-Skepticism-Data-Driven-World/dp/0525509186",
    "type": "book",
    "platform": "",
    "language": "English",
    "tags": [
      "statistics",
      "analytics",
      "data science",
      "mathematics",
      "analytical thinking"
    ],
    "status": "to-read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "",
    "difficulty": "Advanced",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "8/14/2025",
    "dateRead": ""
  },
  {
    "id": "breaking_the_sorting_barrier_for_directed_single_s",
    "title": "Breaking the Sorting Barrier for Directed Single-Source Shortest\r\nPaths",
    "author": "Ran Duan ∗,\r\nJiayi Mao ∗, Xiao Mao †,  Xinkai Shu, ‡ Longhui Yin ∗",
    "link": "https://arxiv.org/pdf/2504.17033",
    "type": "article",
    "platform": "",
    "language": "English",
    "tags": [
      "algorithm",
      "analysis",
      "Dijkstra's algorithm",
      "shortest path",
      "record break"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "The first result to break the O(m + n log n) time bound of Dijkstra’s algorithm on sparse graphs showing that it is not optimal for SSSP. Gives a deterministic O(m log2/3 n)-time algorithm for single-source shortest paths (SSSP) on directed graphs with real non-negative edge weights in the comparison-addition model\n\n1. Finds key vertices to reduce the frontiers (set of vertices you have reached in the current layer)\n\n2. Uses Bellman-Ford-style relaxations (\"checking if the currently known distance to a vertex can be improved by going through another vertex and if so, update it\" - ChatGPT)\n\n3. Performs partial Bread First Search\n\nThis algorithm doesn't track all the vertices, it only keeps a subset of these vertices.",
    "quotes": [],
    "impact": null,
    "length": "",
    "difficulty": "Advanced",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "8/15/2025",
    "dateRead": "08/15/2025"
  },
  {
    "id": "statistical_methods:_basic_concepts,_interpretatio",
    "title": "Statistical methods: Basic concepts, interpretations, and cautions",
    "author": "Sander Greenland, Professor Emeritus \r\nDepartment of Epidemiology and Department of Statistics \r\nUniversity of California, Los Angeles",
    "link": "https://arxiv.org/pdf/2508.10168",
    "type": "article",
    "platform": "",
    "language": "English",
    "tags": [
      "statistics",
      "inference",
      "interpretation",
      "hypothesis testing",
      "confidence"
    ],
    "status": "in-progress",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "08/18/2025",
    "dateRead": ""
  },
  {
    "id": "ai_and_memory_wall",
    "title": "AI and Memory Wall",
    "author": "Amir Gholami1,2 Zhewei Yao1 Sehoon Kim1 Coleman Hooper1 Michael W. Mahoney1,2,3 Kurt Keutzer1\r\n1University of California, Berkeley 2\r\nICSI 3LBNL",
    "link": "https://arxiv.org/pdf/2403.14123",
    "type": "article",
    "platform": "",
    "language": "English",
    "tags": [
      "ai",
      "memory bound",
      "bottleneck",
      "run-time",
      "decoder models",
      "deep learning"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "1. One needs to study the arithmetic intensity of the operations involved. Arithmetic intensity is the number of FLOPs that can be performed per byte loaded from memory. Computed by dividing the total number of FLOPs by the total number of bytes accessed (also referred to as MOPs, or memory operations).\n2. Memory wall can become a major bottleneck for decoder models (at low batch sizes) and not compute.",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "",
    "relatedTo": [],
    "personalReflection": "Memory wall/bounds with large batch-size lead to sharp minimum, that is, spiky loss curves and may reduce generalization.",
    "citationFormat": "",
    "dateAdded": "08/19/2025",
    "dateRead": "08/19/2025"
  },
  {
    "id": "breaking_the_memory_wall:_a_study_of_i/o_patterns_",
    "title": "Breaking the Memory Wall: A Study of I/O Patterns and GPU\r\nMemory Utilization for Hybrid CPU-GPU Offloaded Optimizers",
    "author": "Avinash Maurya, \nJie Ye, M. Mustafa Rafique,\nFranck Cappello,\nBogdan Nicolae,",
    "link": "https://arxiv.org/pdf/2406.10728v1",
    "type": "article",
    "platform": "",
    "language": "English",
    "tags": [
      "optimization",
      "memory-bound",
      "deep learning",
      "ai",
      "llms"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "It has revealed several important bottlenecks of ex\u0002isting implementations: \n\n1. redundant and/or blocking device-to-host and host-to-device data transfers that slow down the last backward pass and the update step, \n\n2. slow upload of updated model parameters back to the GPUs in half precision, \n\n3. fluctuations of GPU memory utilization and PCIe links that lead to underutilization of GPUs especially during the update step, \n\n4. contention for PCIe links due to 3D parallelism, and contention for the I/O bandwidth of the host memory due to concurrent data transfers and CPU computations.",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "",
    "citationFormat": "",
    "dateAdded": "08/19/2025",
    "dateRead": "08/19/2025"
  },
  {
    "id": "scheduling_deep_learning_jobs_in_multi_tenant_gpu_",
    "title": "Scheduling Deep Learning Jobs in Multi-Tenant GPU Clusters via Wise Resource Sharing",
    "author": "Yizhou Luo∗\r\n, Qiang Wang†\r\n, Shaohuai Shi†\r\n, Jiaxin Lai∗\r\n, Shuhan Qi†\r\n, Jiajia Zhang†\r\n, Xuan Wang†\r\nHarbin Institute of Technology (Shenzhen)\r\nGuangdong Provincial Key Laboratory of Novel Security Intelligence Technologies",
    "link": "https://arxiv.org/pdf/2407.13088",
    "type": "article",
    "platform": "",
    "language": "English",
    "tags": [
      "deep learnig",
      "job scheduling",
      "memory contention",
      "resource utilization"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "1. Introduces a novel DDL job scheduling model enabling multiple jobs to fully or partially share the same set of GPUs while ensuring model convergence through gradi\u0002ent accumulation. Unlike existing methods that increase batch size and GPU numbers to enhance performance, risking accuracy degradation, our model focuses on GPU resource sharing across jobs and mitigates GPU memory constraints through gradient accumulation, thereby poten\u0002tially reducing queuing time for waiting DDL jobs.\n\n2. SJF-BSBF (shortest job first with best shar\u0002ing benefit first) reduces average job com\u0002pletion time by 27-33%.  \n\n3. Given that GPU memory constraints may limit the per-GPU batch size, some schedulers tackle this limitation through memory offloading (which may introduce additional system overhead) or by adjusting batch sizes and other training hyper-parameters (which may compromise model accuracy).\n\n4. Adopts the ”gang-scheduling” discipline widely prevalent in practical large-scale GPU clusters.\n\n5. Different jobs exhibit varying sensitivities to network communication and GPU workloads.\n\n6. Once the free GPU number is not enough to execute the job, they manage to look for those already occupied by the running jobs to schedule the new one. This is the core logic of SJF-BSBF.",
    "quotes": [],
    "impact": null,
    "length": "Short",
    "difficulty": "Intermediate",
    "relatedTo": [],
    "personalReflection": "GPU contention can negatively impact model convergence and final accuracy. The authors have shown that this can be mitigated using a non-preemptive scheduling heuristic (SJF-BSBF) that dynamically adjusts sub-batch sizes and sharing timings while ensuring both efficient GPU sharing and maintenance of Deep Learning model accuracy via gradient accumulation.",
    "citationFormat": "",
    "dateAdded": "08/19/2025",
    "dateRead": "08/19/2025"
  },
  {
    "id": "prompt_injection:_a_visual,_non_technical_primer_f",
    "title": "Prompt injection: A visual, non technical primer for ChatGPT users",
    "author": "Georg Zoeller",
    "link": "https://www.linkedin.com/pulse/prompt-injection-visual-primer-georg-zoeller-tbhuc/",
    "type": "linkedin_post",
    "platform": "linkedIn",
    "language": "English",
    "tags": [
      "chat gpt",
      "prompt injection",
      "llms",
      "ai"
    ],
    "status": "read",
    "rating": null,
    "ratingDescription": "",
    "keyInsights": "LLMs, like many transformer based systems only have one input which must carry both the instructions and data into the depth of the model, where the determination is made in ways we have very little influence on. \r\n\r\nThis is an architectural limitation that has not budged since 2017, all agents are vulnerable to it on some level and there's no major credible breakthroughs on the horizon to reliably fix it.",
    "quotes": [],
    "impact": 5,
    "length": "Short",
    "difficulty": "Intermediate",
    "relatedTo": [
      "llms"
    ],
    "personalReflection": "Prompt injection can easily tweak the reasoning models into false conclusions. This is very evident in all the Agents based systems too. The question arises also about the security models like Co-Pilot was found leaking internal documents and CRM data. That concludes with the fact that every AI app is prone to high security loop hole. Any context engineer can very easily delude the halucinating AI models.",
    "citationFormat": "Q: Wait, doesn't that mean if I use ChatGPT or Perplexity to do research on the internet ... any page can change my instructions (\"Find the best...\"),  \n\nA: Sure it does. We've had a persistent prompt injection in our demo page, ai-ceo.org for a while which looks like this: So yes, Perplexity is kind of crap, it will just parrot whatever the page tells it to and produce a bunch of unrelated citations to make it look legitimate.\n\n Q: So it's really unfixable \n\nA: With current architecture yes. It is mitigatable, to some extend and if compute gets cheap enough (maybe around 100x more), I can imagine companies spending some of that budget for more defense at depth (\"panel of judges model\").  Personally I wouldn't suggest starting any projects under the assumption this will be fixable anytime soon. It's been like this for the last 7 years or so...|I will update this document should any major breakthroughs or seriously viable mitigation strategies emerge.",
    "dateAdded": "08/20/2025",
    "dateRead": "08/20/2025"
  }
];

export const getReadingStats = (data) => {
  const readItems = data.filter(item => item.status === 'read').length;
  const inProgress = data.filter(item => item.status === 'in-progress').length;
  const toRead = data.filter(item => item.status === 'to-read').length;
  
  const ratedItems = data.filter(item => item.rating !== null);
  const averageRating = ratedItems.length > 0 
    ? (ratedItems.reduce((sum, item) => sum + item.rating, 0) / ratedItems.length).toFixed(1)
    : 0;
    
  const impactItems = data.filter(item => item.impact !== null);
  const averageImpact = impactItems.length > 0
    ? (impactItems.reduce((sum, item) => sum + item.impact, 0) / impactItems.length).toFixed(1)
    : 0;

  return {
    totalItems: data.length,
    readItems,
    inProgress,
    toRead,
    averageRating: parseFloat(averageRating),
    averageImpact: parseFloat(averageImpact)
  };
};

export const getTagFrequency = (data) => {
  const tagCounts = {};
  data.forEach(item => {
    item.tags.forEach(tag => {
      tagCounts[tag] = (tagCounts[tag] || 0) + 1;
    });
  });
  return tagCounts;
};
